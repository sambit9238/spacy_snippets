{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy_lexeme_doc_span.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMd2hxpPF9B/1LfgfwuJLLP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgDDjTR0l9HO"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSx1-J5JmTwd"
      },
      "source": [
        "**Vocab**   :   stores data shared across multiple documents.\n",
        "\n",
        "**String store** :   lookup table in both directions\n",
        "\n",
        "To save memory, spaCy encodes all strings to hash values. Strings are only stored once in the StringStore via nlp.vocab.strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0-7q2KCleQ3",
        "outputId": "9c0cdf69-4310-4d89-c430-65ab4073361f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
        "coffee_hash"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3197928453018144401"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEzEf000mPe7",
        "outputId": "73572b14-f7f1-417b-ef63-47fedd927d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#Hashes can't be reversed – that's why we need to provide the shared vocab\n",
        "# Raises an error if we haven't seen the string before\n",
        "string = nlp.vocab.strings[3197928453018144401]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d3b3a1bd4c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Hashes can't be reversed – that's why we need to provide the shared vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Raises an error if we haven't seen the string before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3197928453018144401\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '3197928453018144401'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF7fV5_Zmrgc",
        "outputId": "203e632a-1463-4fa4-ac75-69e69adfacd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "doc = nlp(\"I love coffee\")\n",
        "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
        "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 3197928453018144401\n",
            "string value: coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnO61KD5m4sy",
        "outputId": "3ba51bab-6d8f-4bd7-b65e-9093b26aad5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#doc can expose vocabs too\n",
        "doc.vocab.strings[\"coffee\"]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3197928453018144401"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a43FVzVQnRgD"
      },
      "source": [
        "**Lexeme** object is an entry in the vocabulary\n",
        "\n",
        "It only contain non context specific"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjVQ_b3wnGia",
        "outputId": "4fa329eb-0399-452a-bdad-a57ab4cbf64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc = nlp(\"I love coffee too.\")\n",
        "lexeme = nlp.vocab[\"too\"]\n",
        "\n",
        "# Print the lexical attributes\n",
        "print(lexeme.text, lexeme.orth, lexeme.is_alpha, lexeme.is_stop)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "too 12286903790479710773 True True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ju51D8ojnR"
      },
      "source": [
        "The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the nlp object. But you can also instantiate the class manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHauegxAnZUY"
      },
      "source": [
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0rWjJq8onvd",
        "outputId": "0975bedd-3aac-40eb-8888-f2e37681b9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc.text"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello world!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cISsmEguoo3I",
        "outputId": "400f4801-21dc-4db8-f2cc-2c3751cf217c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Span\n",
        "\n",
        "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
        "spaces = [True, True, True, False]\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words, spaces)\n",
        "print(doc.text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like David Bowie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jFrvyCApLvc",
        "outputId": "20b2ab01-a5c2-4ef2-a0d9-6c59e9ac9dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span = Span(doc, 2, 4, label=\"PERSON\")\n",
        "print(span.text, span.label_)\n",
        "\n",
        "# Add the span to the doc's entities\n",
        "doc.ents = [span]\n",
        "\n",
        "# Print entities' text and labels\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "David Bowie PERSON\n",
            "[('David Bowie', 'PERSON')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QXTcKGaqIHg"
      },
      "source": [
        "# Good Practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXsCk35mpUPe"
      },
      "source": [
        "doc = nlp(\"Berlin looks like a nice city\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8stDu9DmqLtJ",
        "outputId": "a386175f-1647-496d-a7e7-78354a735ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Get all tokens and part-of-speech tags\n",
        "token_texts = [token.text for token in doc]\n",
        "pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "for index, pos in enumerate(pos_tags):\n",
        "    # Check if the current token is a proper noun\n",
        "    if pos == \"PROPN\":\n",
        "        # Check if the next token is a verb\n",
        "        if pos_tags[index + 1] == \"VERB\":\n",
        "            result = token_texts[index]\n",
        "            print(\"Found proper noun before a verb:\", result)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found proper noun before a verb: Berlin\n",
            "CPU times: user 178 µs, sys: 0 ns, total: 178 µs\n",
            "Wall time: 154 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TtIEJSnqOex",
        "outputId": "5818214b-7d2f-494f-b19d-01fd1648972d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "# Iterate over the tokens\n",
        "for token in doc:\n",
        "    # Check if the current token is a proper noun\n",
        "    if token.pos_ == \"PROPN\":\n",
        "        # Check if the next token is a verb\n",
        "        if doc[token.i + 1].pos_ == \"VERB\":\n",
        "            print(\"Found proper noun before a verb:\", token.text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found proper noun before a verb: Berlin\n",
            "CPU times: user 108 µs, sys: 0 ns, total: 108 µs\n",
            "Wall time: 113 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtiUhcLwqZxI"
      },
      "source": [
        "* Always convert the results to strings as late as possible, and try to use native token attributes to keep things consistent.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0gkgxUnqSPL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}